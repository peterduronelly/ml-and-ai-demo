{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "188636dd-7e87-497f-af15-0da8e4dbec85",
   "metadata": {},
   "source": [
    "## In this session\n",
    "- How to manage model complexity?\n",
    "- How to make the machine learn?\n",
    "\n",
    "## Topics covered\n",
    "\n",
    "- cross-validation\n",
    "- grid search\n",
    "- lasso\n",
    "- random forest\n",
    "- gradient boosting machines\n",
    "- neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5008dbcc-4982-440a-92bc-b7c0548b86f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import patsy\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, GridSearchCV, train_test_split, cross_val_score, cross_val_predict, RandomizedSearchCV\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import warnings\n",
    "import mglearn\n",
    "from IPython import display\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ddf2d-1fee-4e22-9720-209dfea7bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'airbnb_.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e60f32-60ab-4b10-b96d-ad91318bc80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join('datasets', filename)\n",
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6dea1a-3d95-4af4-96bc-b95650a9a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03df20f-f73b-4c98-98e2-f322842bcce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbebe838-64e6-46a2-838a-2f30a08a8db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.price < 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f9998b-4893-4a44-8381-68e377edf46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.price.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407286c1-abec-42fc-a8a7-cf9434b26637",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.price.quantile([0.01, 0.1, 0.25, 0.5, 0.75, 0.90,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45452af9-f6b4-424b-84fb-9c09a2d11396",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.price.plot(kind = 'hist', bins = 25, rwidth = 0.9, title = 'price');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780c9d88-cb1b-4bcd-a2ea-38123405c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ln_price.plot(kind = 'hist', bins = 25, rwidth = 0.9, title = 'log price');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c45273-ee15-4afc-8cb0-9d16db3a2d02",
   "metadata": {},
   "source": [
    "### Problem statement\n",
    "\n",
    "- Find a good fit...\n",
    "- while controlling for complexity...\n",
    "- without overfitting to the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5e8958-315f-46ec-b639-83d423efd819",
   "metadata": {},
   "source": [
    "#### Controlling for complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7693da-24c6-4386-9605-100a6e562024",
   "metadata": {},
   "source": [
    "Find a good balance between good fit (low RMSE) and complex model (many variables) &rarr; add `penalty` to RMSE. Our target is to **minimize**\n",
    "<center>\n",
    "    \n",
    "    error + penalty\n",
    "    \n",
    "</center>\n",
    "\n",
    "The simplest definition:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    $RMSE + \\lambda * (\\sum\\left\\lvert\\beta_j\\right\\lvert ) \\xrightarrow{} min$\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "where $beta_j$ is the parameter of the $j^{th}$ explanatory variable and $\\lambda$ denotes the amount of *shrinkage* in the regression equation. \n",
    "<br>\n",
    "\n",
    "We run many broad regressions with a lot of explanatory variables and try to get rid of those parameters which are not important in identifying a robust pattern. We call this *Least Absolute Shrinkage and Selection Operator*, or <bold>`lasso`</bold>.\n",
    "\n",
    "### Questions: \n",
    "##### 1. What is the right $\\lambda$ which gives us a robust fit?\n",
    "##### 2. How can we take out-of-sample performance into account *while* building our models?\n",
    "##### 3. How can we compare the performance of various models while controlling for overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a50e68-2514-4d7c-80b6-5d520e9453dc",
   "metadata": {},
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e362a386-9ed9-4caf-8587-736b4c3d41c0",
   "metadata": {},
   "source": [
    "#### 1. Finding the proper $\\lambda$: `grid search`\n",
    "\n",
    "There are parameters which are not learnt during the estmation process. These parameters are set by trial and error through an iteration on predefined set of values. \n",
    "\n",
    "We usally provide a set of values as possible $\\lambda$s, estimate our models with each of them, and pick the one which minimizes `error + penalty`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bb6f3a-dddc-4d84-b30b-fc63a7ce28ac",
   "metadata": {},
   "source": [
    "#### 2. Considering out-of-sample perfomance when building models: `cross-validation`\n",
    "\n",
    "With cross-validation (CV) we split our training dataset to `n` pieces. We do *n* estimation steps and in each step we train the model on *(n-1)/n*th fraction of the data and check model performance (*rmse*) on the remaining `n`th fraction. We rotate the subsamples so that each observation will be a training data point `n-1` times and measurement point one time.\n",
    "\n",
    "We call each iteration a `fold`, and the process is an `n-fold cross-validation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b457e50-7d45-47db-8b23-eae33442dd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_cross_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a869923e-ef90-43fd-aa3a-9d964be9ece5",
   "metadata": {},
   "source": [
    "#### 3. Comparing model performancees: `train-test split`\n",
    "\n",
    "We split our data to a *training set* and to a *test set*. \n",
    "1. We train our model, using cross-validation, on the training set.\n",
    "2. Then we measure our fit on the test set.\n",
    "3. We do it for all model versions, and then compare the fits (measured on the test set.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b66ae-660a-4879-a20e-9781ed69738e",
   "metadata": {},
   "source": [
    "### Model & data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e6c624-ddf1-4ade-b0a2-111eaf1eb097",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_lev = [\n",
    "    \"n_accommodates\",\n",
    "    \"n_beds\",\n",
    "    \"n_days_since\",\n",
    "    \"f_property_type\",\n",
    "    \"f_room_type\",\n",
    "    \"f_bathroom\",\n",
    "    \"f_cancellation_policy\",\n",
    "    \"f_bed_type\",\n",
    "    \"f_neighbourhood_cleansed\"\n",
    "]\n",
    "reviews = [\"f_number_of_reviews\", \"n_review_scores_rating\", \"flag_review_scores_rating\"]\n",
    "poly_lev = (\"n_accommodates2\", \"n_days_since2\", \"n_days_since3\")\n",
    "# not use p_host_response_rate due to missing obs\n",
    "amenities = list(df.filter(regex=\"^d_.*\"))\n",
    "X1 = (\"n_accommodates:f_property_type\",\n",
    "    \"f_room_type:f_property_type\",\n",
    "    \"f_room_type:d_familykidfriendly\",\n",
    "    \"d_airconditioning:f_property_type\",\n",
    "    \"d_cats:f_property_type\",\n",
    "    \"d_dogs:f_property_type\")\n",
    "X2=(\"f_property_type:f_neighbourhood_cleansed\",\n",
    "    \"f_room_type:f_neighbourhood_cleansed\",\n",
    "    \"n_accommodates:f_neighbourhood_cleansed\")\n",
    "X3=\"(f_property_type + f_room_type + f_cancellation_policy + f_bed_type) * (\"+ \"+\".join(amenities) +\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bfbdf3-689a-49dd-9a62-daba79324643",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.f_cancellation_policy.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6277742-d71b-45bc-b8e7-28dc6d265dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.f_neighbourhood_cleansed.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d800ca7-0262-46fc-abb9-ed26f4610b4c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "![](https://www.grants.londoncouncils.gov.uk/images/boroughmap.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1121690e-a96a-4927-8c7d-5bc691e96833",
   "metadata": {},
   "outputs": [],
   "source": [
    "amenities[4:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7091e0-457d-401c-ac89-6fbcd66aefc6",
   "metadata": {},
   "source": [
    "- Create train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343984d9-4a93-4fdc-9673-ca967533884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size= 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4383da01-92f6-4aa8-bbd3-27e3a6e9e548",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73df579-447f-4351-a952-2f52c29bafd8",
   "metadata": {},
   "source": [
    "### Baseline: linear regression (broad model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24410cd-bb8c-4164-b4d6-39b7df3f88e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars =\" ~ n_accommodates + f_neighbourhood_cleansed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b65445-8bf7-404e-a909-49609feb18fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = patsy.dmatrices('price' + vars, df_train)\n",
    "y_test, X_test = patsy.dmatrices('price' + vars, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c33e873-d6ed-4c2f-aad8-f0d5d8b78e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmse = pd.DataFrame(columns = ['training set RMSE', 'test set RMSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17881976-c6a4-4257-a9f2-d73442ea01df",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a675495-d65d-4189-a334-c90f7b391c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(lin_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b1b0cb-8e56-4ba8-90f0-f9fc94221bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_fitted_train_lin_reg = lin_reg.predict(X_train)\n",
    "price_fitted_test_lin_reg = lin_reg.predict(X_test)\n",
    "\n",
    "rmse_train_lin_reg = mean_squared_error(y_train, price_fitted_train_lin_reg, squared= False)\n",
    "rmse_test_lin_reg = mean_squared_error(y_test, price_fitted_test_lin_reg, squared= False)\n",
    "\n",
    "print('\\nTrain RMSE: {:,.2f}.'.format(rmse_train_lin_reg))\n",
    "print('Test RMSE: {:,.2f}.\\n'.format(rmse_test_lin_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205ad801-fb1a-4b01-80ca-07d5529148e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmse.loc['linear regression'] = [rmse_train_lin_reg, rmse_test_lin_reg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0ca6b2-e796-4199-95c8-13d729b27f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ceee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame({'variable': X_train.design_info.column_names, 'coef': lin_reg.coef_[0]}).to_string(formatters={'coef':'{:,.2f}'.format}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44118f50-09c4-4edb-abf4-a1a9f339a947",
   "metadata": {},
   "source": [
    "#### Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755147d7-d741-47b2-81f0-ee441c75f3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba7b4ec-1fb4-4843-a84b-e7d217f0c9a1",
   "metadata": {},
   "source": [
    "#### Visual representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36977b6b-dcd7-48c3-9aaf-65b294eadb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize = (12,6))\n",
    "axs[0].scatter(x = y_train, y = price_fitted_train_lin_reg, marker = '.', color = 'black')\n",
    "axs[0].axline([0, 0], [1, 1], color = 'k')\n",
    "axs[0].set_title('train')\n",
    "axs[0].set_xlim(0,500)\n",
    "axs[0].set_ylim(0,500)\n",
    "axs[1].scatter(x = y_test, y = price_fitted_test_lin_reg, marker = '.', color = 'k')\n",
    "axs[1].axline([0, 0], [1, 1], color = 'k')\n",
    "axs[1].set_xlim(0,500)\n",
    "axs[1].set_ylim(0,500)\n",
    "axs[1].set_title('test')\n",
    "fig.suptitle('Original vs predicted values - linear regression')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='original', ylabel='fitted/predicted')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2287f277-a015-47e1-88af-0db13f3cc065",
   "metadata": {},
   "source": [
    "### ML 'light': Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a622b3-76d6-488c-b8ee-9c2fc2277052",
   "metadata": {},
   "source": [
    "- Prepare for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b32cea-9861-49d4-a8c8-1dc61c515fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars =\" ~ \"+\"+\".join(basic_lev)+\"+\"+\"+\".join(reviews)+\"+\"+\"+\".join(poly_lev)+\"+\"+\"+\".join(X1)+\"+\"+\"+\".join(X2)+\"+\"+\"+\".join(amenities) # +\"+\"+X3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72781a6e-b6b1-4364-9db0-c0b0b917a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb601e69-6df4-4aa6-98e0-6b46f281638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = patsy.dmatrices('price' + vars, df_train)\n",
    "y_test, X_test = patsy.dmatrices('price' + vars, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10e710a-feb4-4137-8547-fac4c4f26ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of columns in the broad model: {len(X_train.design_info.column_names)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d346c41-1566-4242-83f8-57316a22fda2",
   "metadata": {},
   "source": [
    "- Instantiate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524f81e1-c63a-4ffa-a5c2-fc1b1de18f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = Lasso()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a887d37-5cd0-43bb-ae18-87924e2e3230",
   "metadata": {},
   "source": [
    "- Define a set of alphas (aka lambdas). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad43d93-4719-4353-bc6e-691df5375d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_grid = dict()\n",
    "tune_grid['alpha'] = np.arange(0.05, 1, 0.05) # Just to confuse the reader, Lasso's lambda is called 'alpha'. Why? Because 'lambda' is a reserved word in Python.\n",
    "tune_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de779ab3-6373-450e-9a68-4f1184b2a449",
   "metadata": {},
   "source": [
    "- Define cross-validation.\n",
    "- Define grid search.\n",
    "- Fit model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d7558f-b20b-4a2c-828b-85db0e6686c8",
   "metadata": {},
   "source": [
    "`GridSearchCV` not only searches for the best parameters, but also automatically fits a new model on the whole training dataset with the parameters that yielded the best cross-validation performance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c51e4a6-5c56-49f3-9154-3b7c51c1fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedKFold(n_splits = 4, n_repeats= 1, random_state = 20240523)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator = lasso_model, \n",
    "    param_grid = tune_grid, \n",
    "    scoring = 'neg_root_mean_squared_error', \n",
    "    cv = cv, \n",
    "    verbose = 3)\n",
    "\n",
    "lasso_reg = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158b7ef7-a158-497f-9272-eca8a701385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dae96e-3b92-4747-b992-e62d2a673239",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nonzero(lasso_reg.best_estimator_.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8499e-bf3c-408e-bb82-1df587661729",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lasso_coefs = pd.DataFrame({'variable': X_train.design_info.column_names, 'coefficient': lasso_reg.best_estimator_.coef_})\n",
    "df_lasso_coefs[df_lasso_coefs.coefficient > 0].sort_values('coefficient', ascending = False).reset_index(drop = True).iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73992f92-ea09-42f8-9e4a-0fd0c06100ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_fitted_train_lasso_reg = lasso_reg.predict(X_train)\n",
    "price_fitted_test_lasso_reg = lasso_reg.predict(X_test)\n",
    "\n",
    "rmse_train_lasso_reg = mean_squared_error(y_train, price_fitted_train_lasso_reg, squared= False)\n",
    "rmse_test_lasso_reg = mean_squared_error(y_test, price_fitted_test_lasso_reg, squared= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f2bf8d-4da7-48e0-a6d3-85065ecc12a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_train_lasso_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176c7621",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmse.loc['lasso regression'] = [rmse_train_lasso_reg, rmse_test_lasso_reg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79496e6c-4d5b-49cb-b408-c9f51ea2bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fc454b-998a-4a76-9a01-3c855c7d5ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize = (12,6))\n",
    "axs[0].scatter(x = y_train, y = price_fitted_train_lasso_reg, marker = '.', color = 'black')\n",
    "axs[0].axline([0, 0], [1, 1], color = 'k')\n",
    "axs[0].set_title('train')\n",
    "axs[0].set_xlim(0,500)\n",
    "axs[0].set_ylim(0,500)\n",
    "axs[1].scatter(x = y_test, y = price_fitted_test_lasso_reg, marker = '.', color = 'k')\n",
    "axs[1].axline([0, 0], [1, 1], color = 'k')\n",
    "axs[1].set_xlim(0,500)\n",
    "axs[1].set_ylim(0,500)\n",
    "axs[1].set_title('test')\n",
    "fig.suptitle('Original vs predicted values - lasso regression')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='original', ylabel='fitted/predicted')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a62a0e7-5074-4c0f-bc6a-44d556a6ce90",
   "metadata": {},
   "source": [
    "### CART: Classification and Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcde8125-8df2-43b2-b77a-9051e73c21da",
   "metadata": {},
   "source": [
    "The basic idea of a regression tree is **splitting** the dataset into small **bins** by the values of the explanatory ($x$) variables, and predicting $y$ as the average value of $\\hat y$ within those bins. Creating a regression tree is called *building* or *growing a tree*. The algorithm has no formula.\n",
    "\n",
    "Growing a tree is stepwise process. We start with a root node, which all the observations. The method uses a search algorithm to find the best $x$ varible to split the root node into two nodes which are as different from each other as possible. Then we split thse two nodes into 2x2 nodes by the same fashion. In theory the algorithm would only stop when all observations are in different bins so we introduce some stopping rule. We can set the maximum level of the tree, the number of final, or *terminal leaves*, the minimum number of observiations in the terminal leaves, or the minimum amount of improvement in our predcition error.\n",
    "\n",
    "\n",
    "![](https://www.tutorialandexample.com/wp-content/uploads/2019/10/Decision-Trees-Root-Node.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ff365-a720-4a0d-9fb0-9412f4af72fb",
   "metadata": {},
   "source": [
    "Trees are very prone to overfitting and they are very short-sighted: at every step they only consider the result of the next step, despite of the fact that each split will affect the possibilities of all subsequent splits. For this reason we never use single trees but they are basic building blocks of other, more effective pattern recognition algorithms.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137bbe06-9226-4fea-a119-3ad2c0018aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_animal_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be68725-0975-48ed-9340-6e6b54a50a0e",
   "metadata": {},
   "source": [
    "#### Building a regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7462e0-12a7-47a3-99dd-cb855ac0bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars =\" ~ \"+\"+\".join(basic_lev)+\"+\"+\"+\".join(reviews)+\"+\"+\"+\".join(amenities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d5dfab-a1fe-44ea-9d11-e3383897d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = patsy.dmatrices('price' + vars, df_train)\n",
    "y_test, X_test = patsy.dmatrices('price' + vars, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef70b7-8d2b-458b-9fd1-6cc66b8784f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_reg = DecisionTreeRegressor(random_state = 20240523, max_depth = 3)\n",
    "cart_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90681a5-08bd-4210-8786-463a601557e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee331fb9-f8dc-47f7-bb62-da0626a745c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b9a92d-4fa4-440f-9b82-482afbc7bfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "tree.plot_tree(cart_reg, filled = True, rounded= True, fontsize = 7)\n",
    "plt.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0b006f-4c59-44fb-b534-27a175579ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'feature': X_train.design_info.column_names, 'importance': cart_reg.feature_importances_}).iloc[[2,3,38,39,42]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98de2381",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_fitted_train_cart_reg = cart_reg.predict(X_train)\n",
    "price_fitted_test_cart_reg = cart_reg.predict(X_test)\n",
    "\n",
    "rmse_train_cart_reg = mean_squared_error(y_train, price_fitted_train_cart_reg, squared= False)\n",
    "rmse_test_cart_reg = mean_squared_error(y_test, price_fitted_test_cart_reg, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f4bce2-74c6-44e0-917e-f9fbf51a8a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmse.loc['cart regression'] = [rmse_train_cart_reg, rmse_test_cart_reg]\n",
    "df_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f4f8d8-2c7b-43d5-baf5-cc679b9a6db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize = (12,6))\n",
    "axs[0].scatter(x = y_train, y = price_fitted_train_cart_reg, marker = '.', color = 'black')\n",
    "axs[0].axline([0, 0], [1, 1], color = 'k')\n",
    "axs[0].set_title('train')\n",
    "axs[0].set_xlim(0,500)\n",
    "axs[0].set_ylim(0,500)\n",
    "axs[1].scatter(x = y_test, y = price_fitted_test_cart_reg, marker = '.', color = 'k')\n",
    "axs[1].axline([0, 0], [1, 1], color = 'k')\n",
    "axs[1].set_xlim(0,500)\n",
    "axs[1].set_ylim(0,500)\n",
    "axs[1].set_title('test')\n",
    "fig.suptitle('Original vs predicted values - cart (decision tree) regression')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='original', ylabel='fitted/predicted')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6042f46a-dde9-4dab-b1e1-209e12e4935f",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "`Random forest` is an `ensemble model`: it creates multiple trees, each of which only losely fits to the data. Each tree only uses \n",
    "- only a handful of the total variables\n",
    "- on a sample of the dataset and\n",
    "- builds only shallow trees.\n",
    "\n",
    "It makes predictions from every single tree and averages them out to come up with the final prediction. Tests show that the ensemble of these *weak learners* gives a more roubst estimate than a single overly precise model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87b3f04-cb0d-46a9-ab9f-e2911f128257",
   "metadata": {},
   "source": [
    "1. **Ensemble Power:** Random forests combine multiple decision trees, each trained on a random subset of features and data points. This diversity leads to more robust and accurate predictions compared to single decision trees.\n",
    "2. **Feature Importance:** Random forests provide insights into feature importance by measuring how much each feature contributes to the overall prediction accuracy. This helps you understand which features are most relevant to your problem.\n",
    "3. **Handling Missing Data:** Random forests can handle missing data gracefully by using techniques like averaging or imputation. This makes them a good choice for real-world datasets that often contain missing values.\n",
    "4. **Non-parametric Nature:** Random forests make no assumptions about the underlying data distribution, making them suitable for a wide range of problems without requiring complex data preprocessing.\n",
    "5. **Scalability:** Random forests can be efficiently trained on large datasets and can handle high-dimensional data with many features. This makes them a powerful tool for big data applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90628d0c-baf5-45e5-86b7-fea6c38c5c96",
   "metadata": {},
   "source": [
    "**Random Forest Regression Parameters**\r\n",
    "\r\n",
    "A random forest regression model boasts several parameters that influence its behavior and performance. Let's explore some of the key ones:\r\n",
    "\r\n",
    "**1. n_estimators:** This parameter controls the number of decision trees in the forest. More trees generally lead to better accuracy, but also increase training time and computational cost. Finding the optimal number through experimentation is crucial.\r\n",
    "\r\n",
    "**2. max_depth:** This parameter limits the maximum depth of each individual decision tree. Deeper trees can capture complex relationships but are prone to overfitting. Setting an appropriate depth helps prevent overfitting and improvesgeneralizability.\r\n",
    "\r\n",
    "**3. min_samples_split:** This parameter determines the minimum number of samples required to split an internal node in a decision tree. A higher value reduces the risk of overfitting by preventing splits based on too few data points.\r\n",
    "\r\n",
    "**4. min_samples_leaf:** This parameter sets the minimum number of samples required to be at a leaf node. A higher value ensures that each leaf node contains enough data to make reliable predictions.\r\n",
    "\r\n",
    "**5. max_features:** This parameter controls the number of features considered at each split in a decision tree. A lower value introduces randomness and helps prevent overfitting, but might also miss important features.\r\n",
    "\r\n",
    "**6. bootstrap:** This parameter determines whether to use bootstrap sampling when building the trees. Bootstrapping involves randomly sampling data points with replacement, creating multiple training sets for the trees. This helps reduce variance and improvegeneralizability.\r\n",
    "\r\n",
    "**7. random_state:** This parameter sets the seed for the random number generator, ensuring reproducibility of results. Using the same random state allows you to compare different models or parameter settings consistently.\r\n",
    "\r\n",
    "**8. criterion:** This parameter defines the function used to measure the quality of a split in a decision tree. Common options include \"mse\" for mean squared error and \"mae\" for mean absolute error. The choice depends on the specific problem and desired outcome.\r\n",
    "\r\n",
    "By carefully tuning these parameters, you can optimize your random forest regression model for your specific task and achieve the best possible performance. Remember, finding the optimal combination often involves experimentation and evaluation on your particular dataset.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268d71de-8e80-4abd-8a74-aa61d1bb64dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars =\" ~ \"+\"+\".join(basic_lev)+\"+\"+\"+\".join(reviews)+\"+\"+\"+\".join(amenities)\n",
    "vars.split('+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e9a9c2-4976-4516-a455-8d6731bfefce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.f_cancellation_policy.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1570beaa-f0c9-4bdd-ae70-d286e05f41cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = patsy.dmatrices('price' + vars, df_train)\n",
    "y_test, X_test = patsy.dmatrices('price' + vars, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9149b8-59d5-42d4-bb33-3d7898123f6a",
   "metadata": {},
   "source": [
    "To find the best combination of these parameter options we use a `grid search`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6544b433-bd39-410e-b44e-fa92e214ddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_grid = {\"max_features\": [4, 6, 8, 10, 12], \"min_samples_leaf\": [5, 10, 15], 'max_depth': [4,5,6]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5fedd-fb18-49d2-bad9-75da9da85326",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rf_model = RandomForestRegressor(random_state = 20240523)\n",
    "grid_search = GridSearchCV(\n",
    "    rf_model,\n",
    "    tune_grid,\n",
    "    cv=4,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    verbose=3,\n",
    ")\n",
    "rf_reg = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0b853c-1986-467d-b932-dea482685124",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c976d2cb-38e7-4485-b1d3-aaf3acd6e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_fitted_train_rf_reg = rf_reg.best_estimator_.predict(X_train)\n",
    "price_fitted_test_rf_reg = rf_reg.best_estimator_.predict(X_test)\n",
    "\n",
    "rmse_train_rf_reg = mean_squared_error(y_train, price_fitted_train_rf_reg, squared= False)\n",
    "rmse_test_rf_reg = mean_squared_error(y_test, price_fitted_test_rf_reg, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2e1ebf-89ca-4be0-8688-43cac62c8fe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf_reg.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424cf067-d332-464b-ae4b-6a59e8cdc649",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_rf_model_cv_results = pd.DataFrame(rf_reg.cv_results_)[[\n",
    "    'param_max_depth', 'param_max_features', 'param_min_samples_leaf', 'mean_test_score']]\n",
    "df_rf_model_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263461e9-c69d-4d2d-9307-ad2cfbb67e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmse.loc['random forest regression'] = [rmse_train_rf_reg, rmse_test_rf_reg]\n",
    "df_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25faefd-bead-4563-8f08-542d2e68a541",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg.best_estimator_.feature_importances_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c35627-fd23-4cc5-869c-ed1d2934d40f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_var_imp = pd.DataFrame(\n",
    "    rf_reg.best_estimator_.feature_importances_, \n",
    "    X_train.design_info.column_names)\\\n",
    "    .reset_index()\\\n",
    "    .rename({\"index\": \"variable\", 0: \"importance\"}, axis=1)\\\n",
    "    .sort_values(by=[\"importance\"], ascending=False)\\\n",
    "    .reset_index(drop = True)\n",
    "\n",
    "df_var_imp['cumulative_importance'] = df_var_imp['importance'].cumsum()\n",
    "df_var_imp[df_var_imp.cumulative_importance < 0.95].style.format({\n",
    "    'imp': lambda x: f'{x:,.1%}',\n",
    "    'cumulative_importance': lambda x: f'{x:,.1%}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9c87c2-cb9f-42da-a34c-d320f0694178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_var_imp[df_var_imp.importance > 0.01]\\\n",
    "    .sort_values(by = 'importance')\\\n",
    "    .plot(kind = 'barh', \n",
    "          x = 'variable', y = 'importance', \n",
    "          figsize = (10,6), grid = True, \n",
    "          title = 'Random forest model highest feature importances', \n",
    "          xlabel = 'variables', legend = False\n",
    "         );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f2a32c-c07e-46ac-aa04-e595e62afc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize = (12,6))\n",
    "axs[0].scatter(x = y_train, y = price_fitted_train_rf_reg, marker = '.', color = 'black')\n",
    "axs[0].axline([0, 0], [1, 1], color = 'k')\n",
    "axs[0].set_title('train')\n",
    "axs[0].set_xlim(0,500)\n",
    "axs[0].set_ylim(0,500)\n",
    "axs[1].scatter(x = y_test, y = price_fitted_test_rf_reg, marker = '.', color = 'k')\n",
    "axs[1].axline([0, 0], [1, 1], color = 'k')\n",
    "axs[1].set_xlim(0,500)\n",
    "axs[1].set_ylim(0,500)\n",
    "axs[1].set_title('test')\n",
    "fig.suptitle('Original vs predicted values - random forest regression')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='original', ylabel='fitted/predicted')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180856c9-b0ac-43e2-b448-894eb90002ab",
   "metadata": {},
   "source": [
    "### Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2087b21e-723f-4a71-80d1-1eb3623fc5ee",
   "metadata": {},
   "source": [
    "`Gradient Boosting Machines` is a powerful machine learning technique that excels at both accuracy and handling complex datasets. Boosting technique follows the concept of ensemble learning, and hence it combines multiple simple models (weak learners or base estimators) to generate the final output. \n",
    "\n",
    "`Boosting` is one of the popular learning ensemble modeling techniques used to build strong classifiers from various weak classifiers. It starts with building a *primary model* from available training data sets then it identifies the *errors* present in the base model. After identifying the error, a secondary model is built, and further, a third model is introduced in this process. In this way, this process of introducing more models is continued until we get a complete training data set by which model predicts correctly.\n",
    "\n",
    "Further, instead of using these models separately to predict the outcome if *we use them in form of series or combination*, then we get a resulting model with correct information than all base models. In other words, instead of using each model's individual prediction, if we use average prediction from these models then we would be able to capture more information from the data. It is referred to as ensemble learning and boosting is also based on ensemble methods in machine learning.\n",
    "\n",
    "![](https://static.javatpoint.com/tutorial/machine-learning/images/gbm-in-machine-learning3.png)\n",
    "\n",
    "Gradient boosting machines consist 3 elements:\n",
    "- loss function\n",
    "- weak learners (simple trees)\n",
    "- additive model (we use every tree for prediction).\n",
    "\n",
    "The main difference between random forest and gradient boosting is that the *trees are not build indepentently but in a sequential fashion*. After the first tree we make a prediction. We calculate the predictions errors and we fit the next tree to the errors! The we predict, measure new errors and make another tree to model the errors. We continue this process until some mechanism (for instance a predefined level of errors, or the number of trees) tells us to stop. \n",
    "\n",
    "The final model here is a stagewise additive model of many individual trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c36c5ee-973a-460f-a6d4-0725105367bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_grid = {\n",
    "    \"max_depth\": [5, 10],\n",
    "    \"learning_rate\": [0.1, 0.2],\n",
    "    \"min_samples_leaf\": [5, 10, 20],\n",
    "    \"ccp_alpha\": [1,5,10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ec91e8-052c-4815-8788-dd1c95cd5e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gbm_model = GradientBoostingRegressor(random_state = 20240523, max_features='sqrt')\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    gbm_model,\n",
    "    tune_grid,\n",
    "    cv=4,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    verbose=10,\n",
    ")\n",
    "\n",
    "gbm_reg = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d08c58-d71a-4475-acb1-705131129d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_reg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c625580-da10-433b-8045-957ad16bc1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_fitted_train_gbm_reg = gbm_reg.best_estimator_.predict(X_train)\n",
    "price_fitted_test_gbm_reg = gbm_reg.best_estimator_.predict(X_test)\n",
    "\n",
    "rmse_train_gbm_reg = mean_squared_error(y_train, price_fitted_train_gbm_reg, squared= False)\n",
    "rmse_test_gbm_reg = mean_squared_error(y_test, price_fitted_test_gbm_reg, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a24022-e04c-4557-bcd0-b917b34867ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmse.loc['gbm regression'] = [rmse_train_gbm_reg, rmse_test_gbm_reg]\n",
    "df_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92830e5-8847-4142-9bed-e84f1f60ee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize = (12,6))\n",
    "axs[0].scatter(x = y_train, y = price_fitted_train_gbm_reg, marker = '.', color = 'black')\n",
    "axs[0].axline([0, 0], [1, 1], color = 'k')\n",
    "axs[0].set_title('train')\n",
    "axs[0].set_xlim(0,500)\n",
    "axs[0].set_ylim(0,500)\n",
    "axs[1].scatter(x = y_test, y = price_fitted_test_gbm_reg, marker = '.', color = 'k')\n",
    "axs[1].axline([0, 0], [1, 1], color = 'k')\n",
    "axs[1].set_xlim(0,500)\n",
    "axs[1].set_ylim(0,500)\n",
    "axs[1].set_title('test')\n",
    "fig.suptitle('Original vs predicted values - GBM regression')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='original', ylabel='fitted/predicted')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3be5500-573d-4f6d-8e3a-0e653e3cda3d",
   "metadata": {},
   "source": [
    "The problem with GBM is that is is **very computation-heavy and difficult to train**. To address that issue Microsoft came up with the [LightGBM](https://github.com/Microsoft/LightGBM) algorithm which excels both in efficieny and accuracy. `LightGBM` splits only one of the nodes, the one with the higher loss.\n",
    "\n",
    "![](https://static.javatpoint.com/tutorial/machine-learning/images/gbm-in-machine-learning4.png)\n",
    "\n",
    "We are using the `LightGBM` model in this demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3459a847-51e6-4cd1-9100-44d562a2d3a6",
   "metadata": {},
   "source": [
    "#### Light GMB parameters\n",
    "\n",
    "**1. max_iter:** maximum number of iterations of the boosting process, i.e. the maximum number of trees.\n",
    "\n",
    "**2. max_depth:** The maximum depth of each tree. The depth of a tree is the number of edges to go from the root to the deepest leaf. Setting an appropriate depth helps prevent overfitting and improves generalizability.\n",
    "\n",
    "**3. max_leaf_nodes:** The maximum number of leaves for each tree.\n",
    "\n",
    "**4. min_samples_leaf:** This parameter sets the minimum number of samples required to be at a leaf node. A higher value ensures that each leaf node contains enough data to make reliable predictions.\n",
    "\n",
    "**5. max_features:** This parameter controls the number of features considered at each split in a decision tree. A lower value introduces randomness and helps prevent overfitting, but might also miss important features.\n",
    "\n",
    "**6. loss:** The loss function to use in the boosting process. Default is squared error. \n",
    "\n",
    "**7. random_state:** This parameter sets the seed for the random number generator, ensuring reproducibility of results. Using the same random state allows you to compare different models or parameter settings consistently.\n",
    "\n",
    "**8. learning_rate:** Learning rate shrinks the contribution of each tree by learning_rate. There is a trade-off between learning_rate and n_estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726a8c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_grid = {\n",
    "    \"max_iter\": [100, 200],\n",
    "    \"max_depth\": [5, 10],\n",
    "    \"learning_rate\": [0.1, 0.2],\n",
    "    \"min_samples_leaf\": [5, 10, 20],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c05c978-72c3-4125-ad1d-a866b0bbeddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lightgbm_model = HistGradientBoostingRegressor(random_state = 20240523)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    lightgbm_model,\n",
    "    tune_grid,\n",
    "    cv=4,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    verbose=10,\n",
    ")\n",
    "\n",
    "lightgbm_reg = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a24419-9c52-4b71-9a9b-9c3c20777320",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_fitted_train_lightgbm_reg = lightgbm_reg.best_estimator_.predict(X_train)\n",
    "price_fitted_test_lightgbm_reg = lightgbm_reg.best_estimator_.predict(X_test)\n",
    "\n",
    "rmse_train_lightgbm_reg = mean_squared_error(y_train, price_fitted_train_lightgbm_reg, squared= False)\n",
    "rmse_test_lightgbm_reg = mean_squared_error(y_test, price_fitted_test_lightgbm_reg, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a458f371-d6a0-41e8-ac7d-f3324cd5f4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmse.loc['light gbm regression'] = [rmse_train_lightgbm_reg, rmse_test_lightgbm_reg]\n",
    "df_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b627dd-c664-4a96-91d7-ae9ae67d044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_reg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e674cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_fitted_train_lightgbm_reg = lightgbm_reg.best_estimator_.predict(X_train)\n",
    "price_fitted_test_lightgbm_reg = lightgbm_reg.best_estimator_.predict(X_test)\n",
    "\n",
    "rmse_train_lightgbm_reg = mean_squared_error(y_train, price_fitted_train_lightgbm_reg, squared= False)\n",
    "rmse_test_lightgbm_reg = mean_squared_error(y_test, price_fitted_test_lightgbm_reg, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bc14a0-6001-420d-a30d-a8f73cc6cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmse.loc['light gbm regression'] = [rmse_train_lightgbm_reg, rmse_test_lightgbm_reg]\n",
    "df_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f581e110-5c3e-4d52-ba0e-5b1d3c61c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize = (12,6))\n",
    "axs[0].scatter(x = y_train, y = price_fitted_train_lightgbm_reg, marker = '.', color = 'black')\n",
    "axs[0].axline([0, 0], [1, 1], color = 'k')\n",
    "axs[0].set_title('train')\n",
    "axs[0].set_xlim(0,500)\n",
    "axs[0].set_ylim(0,500)\n",
    "axs[1].scatter(x = y_test, y = price_fitted_test_lightgbm_reg, marker = '.', color = 'k')\n",
    "axs[1].axline([0, 0], [1, 1], color = 'k')\n",
    "axs[1].set_xlim(0,500)\n",
    "axs[1].set_ylim(0,500)\n",
    "axs[1].set_title('test')\n",
    "fig.suptitle('Original vs predicted values - light GBM regression')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='original', ylabel='fitted/predicted')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f35ba9-3df8-4f6e-97de-f42a1b83b6c4",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb4f679-f0a7-47e9-bc93-74be13b5ea74",
   "metadata": {},
   "source": [
    "An `artificial neural network` (ANN) or a simple traditional neural network aims to solve trivial tasks with a straightforward network outline. An artificial neural network is loosely inspired from biological neural networks. It is a collection of layers to perform a specific task. Each layer consists of a collection of nodes to operate together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad8cdc5-c0fc-40ea-beba-191df5960be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_logistic_regression_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65198471-8b7a-4d26-b8bc-a5e605092db0",
   "metadata": {},
   "source": [
    "These networks usually consist of an `input layer`, one or more `hidden layers`, and an `output layer`. Each node in each network is potentially linked to each node in the preceding and the succceeding layers. While it is possible to solve easy mathematical questions, and computer problems, including basic gate structures with their respective truth tables, it is tough for these networks to solve complicated image processing, computer vision, and natural language processing tasks.\n",
    "\n",
    "For these problems, we utilize `deep neural networks` (DNN), which often have a complex hidden layer structure with a wide variety of different layers. These additional layers help the model to understand problems better and provide optimal solutions to complex projects. A deep neural network has more layers (more depth) than ANN and each layer adds complexity to the model while enabling the model to process the inputs concisely for outputting the ideal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0769594-5ec3-4277-a1ee-78995d1ca863",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_two_hidden_layer_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96dc19e-ffb2-4d4b-9dc3-78d3ace13c26",
   "metadata": {},
   "source": [
    "While neural networks are inspired by biological netowrks, they are fundmanetally different in their architecture.\n",
    "\n",
    "![](https://images.datacamp.com/image/upload/v1707332849/image4_74e3e8d76f.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c18a23-1bd2-420e-b44d-3d644995a09f",
   "metadata": {},
   "source": [
    "A neural network each node is essentially a weighted sum of the nodes in the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80baf35-566d-4543-acc3-39b6b08744a7",
   "metadata": {},
   "source": [
    "This output is given a twist called `activation function`. The activation function introduces nonlinearity into the network to avoid overfitting and, at the same time, allowing it to learn complex patterns in the data.   \n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*XxxiA0jJvPrHEJHD4z893g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcfe776-574a-4785-b112-ddb3ed877867",
   "metadata": {},
   "source": [
    "While the inner workings of neural netowrks can be, and usually is, VERY COMPLICATED, a good demonstration of how they work can be found here: https://goo.gl/ou9iMB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54770dba-da47-4b97-a8cd-2bd6c1d84541",
   "metadata": {},
   "source": [
    "`scikit-learn` uses a `multilayer perceptron model`, a special neural network consisting of fully connected neurons with a nonlinear kind of activation function. \n",
    "\n",
    "There are many more neural network archtitectures for various use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad3bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_grid = {\n",
    "    \"max_iter\": [100, 200],\n",
    "    \"max_depth\": [5, 10],\n",
    "    \"learning_rate\": [0.1, 0.2],\n",
    "    \"min_samples_leaf\": [5, 10, 20],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82799269-5748-47da-b2dc-0e053d6b8dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dnn_model = MLPRegressor(\n",
    "    hidden_layer_sizes= [10,4], \n",
    "    batch_size= 100,\n",
    "    early_stopping= True,\n",
    "    random_state= 20240523)\n",
    "dnn_reg = dnn_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0c6f26-9e56-406b-a394-457bc05c3ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_reg.coefs_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065573b8-18e6-4a5b-b2e5-1ca11c985e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_reg.coefs_[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee7d6e9-b54e-4951-a8c9-555e8661df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_fitted_train_dnn_reg = dnn_reg.predict(X_train)\n",
    "price_fitted_test_dnn_reg = dnn_reg.predict(X_test)\n",
    "\n",
    "rmse_train_dnn_reg = mean_squared_error(y_train, price_fitted_train_dnn_reg, squared= False)\n",
    "rmse_test_dnn_reg = mean_squared_error(y_test, price_fitted_test_dnn_reg, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c10c32c-3a4b-4cd4-bb31-0e190f4f2691",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmse.loc['dnn regression'] = [rmse_train_dnn_reg, rmse_test_dnn_reg]\n",
    "df_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20d4b5c-b0b6-4cfc-9790-01c9bb225f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize = (12,6))\n",
    "axs[0].scatter(x = y_train, y = price_fitted_train_dnn_reg, marker = '.', color = 'black')\n",
    "axs[0].axline([0, 0], [1, 1], color = 'k')\n",
    "axs[0].set_title('train')\n",
    "axs[0].set_xlim(0,500)\n",
    "axs[0].set_ylim(0,500)\n",
    "axs[1].scatter(x = y_test, y = price_fitted_test_dnn_reg, marker = '.', color = 'k')\n",
    "axs[1].axline([0, 0], [1, 1], color = 'k')\n",
    "axs[1].set_xlim(0,500)\n",
    "axs[1].set_ylim(0,500)\n",
    "axs[1].set_title('test')\n",
    "fig.suptitle('Original vs predicted values - deep neural network regression')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='original', ylabel='fitted/predicted')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd819d4-8aad-458f-9f64-0fc5e6bf4fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
